# Service — Этап 4: Post-processing & deployment (docs/5.service.md)

> Назначение: подробное руководство по реализации infer­ence-пайплайна, post-processing, фильтрации похожих брендов и развёртыванию сервисной части проекта. Описывает API (FastAPI), взаимодействие с inference-сервисом, контейнеризацию, мониторинг и практики по производительности.

---

## Содержание

1. Цель сервиса
2. Высокоуровневая архитектура
3. Контракт API (`/detect`) — требования и валидация
4. Preprocessing (на входе)
5. Inference → raw detections
6. Post-processing: NMS, фильтрация, классификация, OCR, VLM
7. Формирование ответа (Pydantic)
8. Ошибки и коды ответа
9. Производительность, таймауты и SLA
10. Развёртывание (Docker, docker-compose, GPU)
11. Observability / Logging / Monitoring
12. Тестирование и CI
13. Security и конфиденциальность
14. Best practices и рекомендации

---

## 1. Цель сервиса

Реализовать надёжный, быстрый и расширяемый REST API на порту `8000` с эндпоинтом `/detect`, который:

* принимает изображение (JPEG, PNG, BMP, WEBP);
* валидирует и преобразует изображение под модель;
* запрашивает inference у model-service;
* выполняет NMS и постобработку (фильтрация `Tinkoff` и пр.);
* возвращает `DetectionResponse` (список bbox в абсолютных координатах).

Сервис должен быть лёгким в поддержке — можно обновлять модель без изменений API.

---

## 2. Высокоуровневая архитектура

Рекомендуемая архитектура — **separation of concerns**:

* **API (FastAPI)** — отвечает за I/O, валидацию, pre/postprocessing, агрегацию результатов и авторизацию.
* **Model-service** — отдельный микросервис с моделью (ONNX/TensorRT) доступный по REST/gRPC (или через Triton/TorchServe).

Преимущества:

* независимое масштабирование сервисов;
* возможность быстро менять/апгрейдить модель;
* минимальный downtime при деплое новых весов.

Типовой flow:

`Client → FastAPI (/detect) → model-service (REST/gRPC) → FastAPI postproc (classifier/OCR/VLM) → Response`.

---

## 3. Контракт API (`/detect`)

**Эндпоинт:** `POST /detect`

**Request:** multipart form с полем `file` (UploadFile). Поддерживаемые форматы: JPEG, PNG, BMP, WEBP. Ограничение размера (configurable) — например 20 MB.

**Response:** `DetectionResponse` (Pydantic):

```py
class BoundingBox(BaseModel):
    x_min: int
    y_min: int
    x_max: int
    y_max: int

class Detection(BaseModel):
    bbox: BoundingBox

class DetectionResponse(BaseModel):
    detections: List[Detection]
```

**Ошибки:** возвращать `ErrorResponse` с понятным описанием.

---

## 4. Preprocessing

Цели: безопасно парсить изображение, сохранять соотношение сторон и подать в модель корректный тензор.

Шаги:

1. **Приём и валидация формата.** Проверить `content_type` и размер.
2. **Декодирование** (Pillow/OpenCV). Если изображение CMYK — конвертировать в RGB.
3. **Resize & pad**: сохранить aspect-ratio. Для моделей с фиксированным входом — использовать letterbox (resize + pad). Параметры (img\_size) брать из конфигурации.
4. **Pre-normalization**: scale `0-255 → 0-1`, либо mean/std normalization по модели.
5. **Return metadata**: оригинальный width/height для конвертации bbox в абсолютные координаты.

Замечание: при работе с большими изображениями использовать tile-cropping (см. раздел Performance) — делить на перекрывающиеся патчи, запускать inference по патчам и мерджить результаты в глобальную систему координат.

---

## 5. Inference → raw detections

FastAPI делает sync/async запрос к model-service:

* **REST**: POST `/infer` с payload = preprocessed image tensor или raw bytes (в зависимости от контракта). Ответ: list of detections `{x,y,w,h,score}` в относительных/абсолютных координатах.
* **gRPC**/Triton: как опция для высокопроизводительного обмена.

В model-service модель должна возвращать: bbox (x\_center,y\_center,w,h or x\_min,y\_min,x\_max,y\_max), confidence и возможно class (мы ожидаем одна класс).

---

## 6. Post-processing: NMS, фильтрация, классификация, OCR, VLM

### 6.1 NMS

Если модель не делает NMS — выполняем NMS с порогом `iou_nms` (например 0.45) и `score_thresh` (например 0.25). Обратите внимание на merging при tile-inference (учёт перекрытия патчей).

### 6.2 Классификация / фильтрация похожих брендов

Для снижения FP (Tinkoff и др.) применить последовательную схему:

1. **Light classifier** (ResNet18/EfficientNet-lite) — быстрый, работает на кропах. Если classifier даёт уверенный `T-Bank` → keep.
2. **OCR** (Tesseract/EasyOCR) — выполняется опционально: если обнаружен текст "Тинькофф" → reject.
3. **VLM verification (Qwen2.5-VL / CLIP)** — fallback для спорных случаев (можно использовать асинхронную проверку в отдельном pipeline).

Правило ансамбля: принять bbox если хотя бы 2 из 3 методов согласны, либо classifier уверенно (`conf > C_high`).

### 6.3 Доп. heuristics

* Размерные фильтры: отбросить нереалистичные по площади bbox (\<min\_area или >max\_area").
* Confidence thresholding: финальная фильтрация по score.

---

## 7. Формирование ответа

1. Преобразовать bbox в абсолютные целые координаты: `x_min = int(round(x_min * orig_w))` и т.д.
2. Клипать значения в границы `0..orig_w/ orig_h`.
3. Вернуть `DetectionResponse(detections=[...])`.

Логирование: сохранять топ-N предсказаний и все FP/FN кейсы на диск/объектное хранилище для анализа.

---

## 8. Ошибки и коды ответа

* `400 Bad Request` — неверный файл/формат.
* `415 Unsupported Media Type` — неподдерживаемый формат.
* `413 Payload Too Large` — превысили лимит размера.
* `500 Internal Server Error` — непредвиденная ошибка (логировать стектрейс).
* `504 Gateway Timeout` — timeout при ожидании model-service.

При ошибке возвращать JSON в формате `ErrorResponse`.

---

## 9. Производительность, таймауты и SLA

* **Время обработки**: ограничение 10s/изображение. Цель: <1s на T4 при TensorRT FP16.
* **Timeouts:**

  * model-service call timeout — 6–8s (configurable)
  * overall request timeout — 9–10s
* **Concurrency:** использовать `uvicorn`/`gunicorn` с worker model (uvicorn workers) и выставить `--workers` = CPU cores (fastAPI front не GPU-bound).
* **Batching:** inference через model-service — batch=1 для минимальной latency.
* **Warm-up:** при старте делать N прогонов через модель, чтобы прогреть CUDA и буферы.

---

## 10. Развёртывание (Docker, docker-compose, GPU)

### 10.1 Dockerfile (FastAPI)

*Базовый образ:* `python:3.10-slim`

Содержимое (пример):

```Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY src/requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY src/ ./src/
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 10.2 docker-compose (локально)

Включить сервисы `api` и `model` (model можно поднимать CPU-режиме локально для тестов). Пример:

```yaml
services:
  api:
    build: ./api
    ports: ["8000:8000"]
    environment: [MODEL_URL=http://model:8501/infer]
  model:
    build: ./model
    deploy: ...
```

### 10.3 GPU notes

* Для model-service использовать `--gpus` и образ с CUDA/TensorRT (или Triton image).
* FastAPI не требует GPU; держать API на CPU для простого горизонтального масштабирования.

---

## 11. Observability / Logging / Monitoring

* **Structured logs** (JSON): request\_id, latency, image\_size, num\_detections, top\_scores.
* **Metrics:** Prometheus metrics: `request_count`, `request_latency_seconds`, `model_latency_seconds`, `detection_count`, `fp_count`, `fn_count`.
* **Tracing:** OpenTelemetry trace связки `api ↔ model-service`.
* **Error reporting:** Sentry or аналог.

---

## 12. Тестирование и CI

* **Unit tests:** preprocessing, bbox conversion, postproc rules.
* **Integration tests:** docker-compose up (api + model CPU) → отправить тестовые изображения, сравнить ответы с ожидаемыми.
* **E2E тесты:** запуск evaluate.py на золотом валид-сете через API (simulate production requests).
* **CI:** GitHub Actions: lint, unit tests, build docker images, smoke integration tests.

---

## 13. Security и конфиденциальность

* Проверять входные файлы: не допускать неожиданных MIME-типы.
* Rate limiting: nginx / API gateway.
* Secrets: хранить ключи (OpenRouter, S3, DB) в environment variables/secret manager.
* TLS: в production — HTTPS через reverse-proxy.

---

## 14. Best practices и рекомендации

* Развязывать API и модель — легко апгрейдить.
* Логировать FP/FN и регулярно создавать retraining-pool.
* Использовать light classifier + OCR для снижения ложных срабатываний похожих брендов.
* При проблемах с tiny-logos — применять tiling на этапе inference.
* Документировать env vars и конфиги в `docs/2.environment.md`.

---

## Приложения (шаблоны)

* `src/main.py` — пример FastAPI-приложения с `/detect` (можем сгенерировать при запросе).
* `docker/Dockerfile` — шаблон как выше.
* `docker-compose.yml` — шаблон для локальной интеграции `api` + `model`.

---

Если нужно, могу сгенерировать готовый `src/main.py` (FastAPI app), `Dockerfile` для API или `docker-compose.yml` для локальной среды. Напиши, что сгенерировать следующим шагом.
