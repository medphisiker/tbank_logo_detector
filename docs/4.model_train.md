# docs/4.model\_train.md

# Этап 3 — Обучение и экспорт модели (Model training & export)

> Назначение: подробное, практическое руководство по выбору архитектуры, тренировочному конвейеру, валидации, экспортy в ONNX/TensorRT и оптимизациям под T4 (16 GB) для задачи одноклассной детекции логотипа **Т-Банка**.

---

## Краткая сводка рекомендаций (one-page)

* **Baseline:** Ultralytics **YOLOv8** (начать с `n`/`s`, финализировать на `m`/`l` при необходимости точности).
* **Input sizes:** для прототипа 640; при проблемах с мелкими логотипами — 960–1280 (или тайлинг, см. ниже).
* **Augmentations:** mosaic, mixup, color jitter, random perspective, cutout, jpeg compression.
* **Оптимизация:** mixed precision (AMP), AdamW, cosine/OneCycle LR; gradient accumulation при отсутствии большой VRAM.
* **Экспорт:** `yolov8` → ONNX → TensorRT (FP16) для запуска на T4.
* **Валидация:** основная метрика — F1 @ IoU=0.5; дополнительно mAP и анализ по размерам объектов.
* **Latency goal:** <10s/изображение (требование). Цель для релиза: <1s (YOLOv8s + TensorRT FP16).

---

## 1. Подготовка к тренировке

### 1.1 Данные и сплиты

* COCO-совместимый JSON; категории: одна (`TBank`).
* Разбивка: train / val (например, 90/10) при условии достаточного объёма; для финальной оценки использовать отдельный **золотой валид-сет** (500–1000 вручную проверенных примеров).
* Для мелких объектов — формировать дополнительный `tile`-набор: разбивать большие изображения на перекрывающиеся патчи (stride < tile\_size) и аннотировать координаты в глобальной системе (важно при сборе GT).

### 1.2 Преобразования / аугментации

Рекомендуемые аугментации:

* Mosaic (особенно полезно для искусственного увеличения плотности объектов).
* MixUp.
* RandomScale / RandomPerspective.
* Color jitter, brightness/contrast, gaussian blur, jpeg compression.
* Cutout / Random Erasing — эмулируют частичное occlusion.
* Для tiny logos — oversample изображений с маленькими логотипами или использовать tile-augmentation.

---

## 2. Рецепты обучения (practical recipes)

### 2.1 Быстрый baseline (эксперимент)

```bash
# пример для ultralytics yolov8 CLI
yolo detect train model=yolov8s.pt data=data.yaml imgsz=640 batch=16 epochs=60 amp=True
```

* `amp=True` — mixed precision;
* `imgsz=640` на старте — быстрее; затем до 960/1280 для финальной тренировки.

### 2.2 Рекомендованный финальный workflow

1. **Pretrain / warm-start** на объединённом датасете (реальные + синтетические).
2. **Freeze backbone** на 3–5 эпох (если используете pretrain), затем `unfreeze` и докачка.
3. **Больше входного разрешения** (960–1280) если много tiny-логотипов — либо использовать тайлинг.
4. **Early stopping / checkpointing** — сохранять лучший чекпойнт по F1\@IoU=0.5.
5. **Hard-negative mining:** после первого цикла собрать FP, добавить в negatives и дообучить.

### 2.3 Гиперпараметры (пример)

* Optimizer: `AdamW`
* LR: `1e-3` (масштабировать под batch: `lr * batch / 64`)
* Scheduler: CosineLR или OneCycle
* Batch: `16` (зависит от VRAM; использовать gradient accumulation при маленьком batch)
* Epochs: `60–150` (для финальных прогонов)
* Weight decay: `0.0005`
* Augment prob: mosaic 0.5, mixup 0.2 (примерно)
* Loss: стандартный YOLOv8 loss (можно пробовать focal box loss для tiny objects)
* Autoanchors: включить (ULTRALYTICS обычно делает это автоматически)

### 2.4 Техники для мелких объектов

* Увеличение `imgsz` (960–1280) — прямой способ, но повышает память и latency.
* **Tile inference / training:** резать изображение на перекрывающиеся патчи (например 640×640 с 20–30% overlap), детектировать на патчах и обратно мержить bbox.
* Увеличить положительный вес для малых bbox (hard-example mining) или использовать focal loss.
* Oversampling малых объектов в батчах.

---

## 3. Логирование и воспроизводимость

* Логи: W\&B / MLflow для трекинга hyperparams, метрик, артефактов.
* Сохранять: best.pt, last.pt, конфиги, seed, requirements.txt.
* Reproducibility: фиксировать seed, версии PyTorch/Ultralytics, CUDA, cuDNN.
* Checkpoints: сохранять промежуточные веса и метаданные (train set hash, commit id).

---

## 4. Валидация / оценки (evaluate.py)

### 4.1 Основная логика метрик

* Для каждой предсказанной bbox найти GT с IoU >= 0.5 (IoU threshold configurable).
* TP: предсказание соответствует ненайденному GT.
* FP: предсказание не соответствует ни одному GT.
* FN: GT без соответствующего предсказания.

Формулы:

```
precision = TP / (TP + FP)
recall    = TP / (TP + FN)
F1        = 2 * precision * recall / (precision + recall)
```

### 4.2 Дополнительно

* mAP\@0.5 и mAP\@0.5:0.95 (если нужно глубокое понимание).
* Ошибочный анализ: разложение по размере (tiny / small / medium), по сцене (наружная реклама / чек / карточка), по освещению.
* Визуализации: overlay GT (зелёный) и pred (красный) с score. Сохранять FP/FN кейсы в отдельную папку.

---

## 5. Экспорт модели (ONNX → TensorRT) и проверка latency

### 5.1 Экспорт в ONNX (Ultralytics)

```bash
# ultralytics export (пример)
yolo export model=runs/detect/train/weights/best.pt format=onnx imgsz=1280 opset=12
# выдаст model.onnx
```

### 5.2 ONNX проверка (ONNX Runtime)

* Прогонять несколько warm-up итераций.
* Измерять latency (median / p99) и peak GPU memory.

### 5.3 TensorRT (FP16) — пример команды

```bash
# trtexec (NVIDIA) конвертирует ONNX в .engine
trtexec --onnx=model.onnx --saveEngine=model_fp16.engine --fp16 --workspace=4096 --verbose
```

* Если нужен динамический shape — генерировать несколько оптимизированных профилей или использовать TensorRT с explicit batch / dynamic shapes.
* Проверить inference time на T4: batch=1, profile realistic input sizes.

### 5.4 Практические советы для T4

* Использовать **FP16** engine (существенно снижает память и ускоряет).
* Batch size = 1 для низкой задержки; для throughput — тестировать большие батчи.
* Warm-up: запускать 10–20 прогонов перед измерением.
* Resize входа до разумного максимума (например max\_side=1600), сохранять соотношение сторон.
* Кешировать model engine в памяти, не пересоздавать для каждого запроса.

---

## 6. Production tips — fast inference & robustness

* **Post-processing:** NMS (iou\_thresh \~0.5), confidence threshold (0.25–0.4) — подобрать по валидации.
* **Filter Tinkoff:** при сомнительных кропах запускать light classifier/OCR/VLM verification (asynchronous or sequential).
* **Timeouts:** ограничивать обработку одного изображения (например 8–10s) и логировать violations.
* **Monitoring:** логировать per-request latency, conf-distribution, FP/FN samples для последующего откладывания в retrain-pool.
* **Edge cases:** добавить pipeline tiling для очень больших изображений и tiny logos.

---

## 7. Что положить в репозиторий (artifacts)

* `train/` — скрипты тренировки, `train.sh`, `data.yaml`, `hyp.yaml`.
* `weights/` — `best.pt`, `last.pt`.
* `export/` — `model.onnx`, `model_fp16.engine` (или ссылка на скачивание, если большой).
* `evaluate.py` — скрипт расчёта Precision/Recall/F1 @ IoU=0.5 + визуализации.
* `benchmarks/` — latency logs, memory usage, тестовые картинки.
* `logs/` — W\&B/MLflow exports или локальные логи.

---

## 8. Чек-лист перед релизом модели

1. Обучили модель на финальном датасете (реальные + синтетика + hard-negatives).
2. Получили reproducible best checkpoint (записаны hyperparams + seed).
3. Экспортировали ONNX → TensorRT FP16 engine и прогнали бенчмарки на T4.
4. Подготовили `evaluate.py` и зафиксировали F1\@IoU=0.5 на золотом валидационном наборе.
5. Подготовили артефакты в репозитории и прописали инструкции в `README.md` (как загрузить веса и engine).

---

Если хотите, могу прямо сейчас сгенерировать:

* пример `train/hyp.yaml` и `data.yaml` для YOLOv8;
* пример `train.sh` и `export.sh` (CLI команды);
* шаблон `evaluate.py` с реализацией подсчёта TP/FP/FN и отрисовкой результатов.
