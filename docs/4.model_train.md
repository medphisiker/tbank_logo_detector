Этап 3 — выбор и тренировка модели

Baseline (быстро): YOLOv8n / YOLOv8s (Ultralytics)

Плюсы: простая тренировка, хорошая скоростьInference, экспорт ONNX, поддержка FP16.

Настройки: input size 640/960, batch 16 (зависит от VRAM), epochs 60–150 с LR-scheduling. Использовать mosaic, mixup, label smoothing.

Класс: одноклассная детекция (T-Bank).

Альтернативы: YOLOX, EfficientDet-D3, Faster-R-CNN (если нужен точнее и есть время).

Для edge/T4: после обучения — экспорт в ONNX → TensorRT (FP16) для inference speed.

Training tips:

Использовать weighted box loss, but default YOLOv8 config OK.

Следить за small object handling (логотипы часто маленькие) — тренировать с multi-scale и увеличить input size, применить focal loss или adjust anchors.

валидация и метрики (скрипт)

Формат: COCO-style GT vs Predictions (или простая CSV).

Метрики: Precision, Recall, F1 at IoU=0.5 (main metric). Также можно считать mAP@0.5:0.95.

Скрипт: для каждой предсказанной bbox — найти GT с IoU>=0.5, подсчитать TP/FP/FN → precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = 2PR/(P+R).

Сохранить визуализации: overlay bbox GT (зелёный) vs pred (красный) + score.

оптимизация производительности (T4, <10s/image)

FP16 inference (Torch/TensorRT) — сильно уменьшает память и ускоряет.

Batch size 1, single-threaded worker for low latency.

Use ONNX → TensorRT engine for maximum throughput on T4.

Resize images to reasonable max dimension (e.g. 1600px max side) with maintained aspect — avoid huge input sizes.

Cache model in GPU memory, warmup before requests.

Aim inference <0.5–1s for YOLOv8s+TensorRT, точно <10s requirement.