# Data Preparation — `docs/3.data_preparation.md`

> Краткое назначение: высокоуровневое, но практическое руководство по подготовке данных для задачи детекции логотипа Т-Банка — от полу-автоматической аннотации до генерации синтетики и валидации. Этот файл описывает **что** и **почему** делать; точные команды/зависимости будут в `annotation/` и `env`-файлах.

---

## Содержание

1. [Overview — идея подхода](#overview)
2. [Автоматический pipeline аннотаций (VLM → SAM → ensemble)](#annotation-pipeline)
3. [Ensemble правила и heuristics](#ensemble)
4. [Фильтрация похожих брендов (Tinkoff) и пост-обработка](#filtering)
5. [Валидация аннотаций: эмбеддинги, кластеризация, золотой валид-сет](#validation)
6. [Синтетика, hard-negatives и аугментации](#synthesis)
7. [Выходные форматы, артефакты и метаданные](#outputs)
8. [Инструменты и полезные ссылки](#tools)
9. [Рекомендованный порядок работ (практический чек-лист)](#checklist)

---

<a name="overview"></a>

## 1. Overview — идея подхода

Цель: минимизировать ручную разметку, получить качественную COCO-аннотацию для обучения детектора и создать воспроизводимую валидацию. Подход:

* Автоматически генерируем proposals текст-запросами с помощью Grounding DINO и/или VLM (Qwen2.5-VL) → уточняем маской через SAM.
* Объединяем (ensemble) результаты разных источников, фильтруем по confidence/IoU/size.
* Применяем пост-фильтрацию похожих логотипов (classifier + OCR + VLM verification).
* Валидируем автоматические аннотации через эмбеддинги (CLIP / DINOv3) и кластеризацию, делаем выборочную ручную проверку; формируем «золотой» валидационный набор (500–1000 изображений).
* Обогащаем датасет синтетикой и hard-negatives, применяем подходящие аугментации.

---

<a name="annotation-pipeline"></a>

## 2. Автоматический pipeline аннотаций (VLM → SAM → ensemble)

### 2.1 Общая последовательность (high-level)

Для каждого изображения:

1. **Grounding DINO** — прогнать с набором промптов (RU/EN) → получить candidate bboxes + confidence.
   Примеры промптов:

   * `"T-bank logo"`
   * `"логотип Т-банка"`
   * `"stylized letter T inside a shield"`
   * `"shield emblem with letter T"`
2. **SAM (Segment Anything Model)** — для каждого bbox передать box prompt → получить маску и более точную рамку.
3. **(Опция)**: дополнительно прогнать **Qwen2.5-VL** (через OpenRouter) с тем же текстовым описанием для подтверждения/оценки bboxes.
4. **Ensemble & heuristics** — слить proposals, применить thresholds и правила объединения (см. раздел 3).
5. Сохранить финальные аннотации в COCO-совместимом формате (bbox + optional mask).

### 2.2 Примечания

* Grounding DINO предназначен для text-grounded detection; SAM даёт качественную маску/корректировку bbox. Синергия этих инструментов («Grounded-SAM») широко применяется для ускорения аннотации.
* Qwen2.5-VL полезен как verifier / second opinion (zero-shot grounding & text-guided verification), особенно у вас есть OpenRouter-доступ.
* Для tiny objects задавайте специальные threshold/size filters (см. ниже).

---

<a name="ensemble"></a>

## 3. Ensemble правила и heuristics

Примеры правил для слияния output’ов нескольких моделей:

* **Keep-if-two**: оставляем bbox, если появилась та же область (IoU >= 0.5) по крайней мере в двух источниках (например, Grounding DINO + Qwen2.5-VL).
* **Confidence threshold**: если bbox от одной модели, но confidence > `C_high` (например 0.85), принимаем.
* **IoU merge**: пересекающиеся proposals (IoU >= 0.6) — объединяем, выбирая bbox как union или weighted average по confidence.
* **Tiny / low confidence rule**: если bbox area < `A_min` (например < 0.01 от площади изображения) и confidence < `C_low` → отправляем в очередь ручной проверки.
* **Size filtering**: отбросить нереалистично большие bbox (логотип обычно не занимает > 50% изображения) или нереалистично маленькие (зависит от задачи).

Параметры `C_high`, `C_low`, `A_min` подбираются эмпирически на первых 2–3k proposals.

---

<a name="filtering"></a>

## 4. Фильтрация похожих брендов (Tinkoff) и пост-обработка

### 4.1 Методы фильтрации

Комбинированный (ensemble) подход даёт наилучшие результаты:

1. **Light classifier**

   * Fine-tune быстрый классификатор (ResNet18 / EfficientNet-lite) на кропах: классы `TBank`, `Tinkoff`, `other`.
   * Это быстрый и недорогой способ отфильтровать большинство ложных срабатываний.

2. **OCR**

   * Выполнить OCR (Tesseract / EasyOCR) вокруг bbox: если найден текст «Тинькофф» → отклонить.
   * Особенно полезно, когда логотип сопровождается брендовым текст.

3. **VLM verification (Qwen2.5-VL / CLIP)**

   * Zero-shot prompt: `"Is this T-Bank logo or Tinkoff?"` — модель выдает вероятности/оценку.
   * CLIP-style similarity: сравнить embedding кропа с эталонными изображениями T-Bank и Tinkoff, выбрать ближайший класс.

4. **Rule ensemble**

   * Пример правила: принять bbox, если хотя бы 2 из 3 методов (detector confidence, classifier, VLM) дают «TBank».

### 4.2 Организация training set для classifier

* Сгенерировать/собрать сбалансированный набор кропов `TBank`, `Tinkoff` и `negatives` (разных размеров/освещения).
* Hard-negative mining: добавить кропы, которые автосистема ошибочно отметила как `TBank`.

---

<a name="validation"></a>

## 5. Валидация аннотаций: эмбеддинги, кластеризация, золотой валид-сет

### 5.1 Кластеризация эмбеддингов

1. Вычислить эмбеддинги для всех proposals (CLIP / DINOv3).
2. Выполнить кластеризацию (k-means / HDBSCAN) — это сгруппирует похожие кропы.
3. Инспектировать *representative samples* из каждого кластера (FiftyOne UI) — быстро выявить массовые ошибки (один большой «плохой» кластер сигнализирует об ошибке pipeline).
4. Для кластеров с высокой внутрикластерной вариативностью — отметить на ручную проверку.

Преимущества: вместо случайной выборки вы проверяете «горячие точки» — экономится время и повышается качество валидации.

### 5.2 Золотой валид-сэт

* Собрать 500–1000 изображений, полностью вручную проверенных и отредактированных (bbox & masks).
* Использовать этот набор для:

  * Итоговой оценки моделей (Precision/Recall/F1 @ IoU=0.5).
  * Тюнинга thresholds и ensemble-правил.
  * Regression testing: каждый релиз модели сравнивать с золотым набором.

---

<a name="synthesis"></a>

## 6. Синтетика, hard-negatives и аугментации

### 6.1 Синтетика

* Рендер векторного логотипа T-Bank в разных цветах/opacity → наклеивание (alpha blending) на случайные фоны (COCO/Unsplash).
* Вариации: разные масштабы, углы поворота, perspective transforms, частичные occlusions (имитация наклейки на поверхность).
* Формировать реальные условия: текстуры, блики, низкое качество JPEG.

Синтетика особенно полезна для повышения вариативности цвета логотипа и агрументации малого набора реальных примеров.

### 6.2 Hard-negatives

* Собрать и пометить похожие логотипы (Tinkoff и другие) и добавить их как negative images (изображения без целевого класса) или отдельный класс для classifier.
* Использовать hard-negative mining: после первой тренировки детектора собрать FP и добавить в train set как negatives.

### 6.3 Аугментации

* Для обучения детектора: rotation, scale, brightness/contrast, blur, jpeg compression, random occlusion/cutout, mosaic/mixup (особенно для YOLO).
* Для classifier: color jitter, small translations, gaussian noise.

---

<a name="outputs"></a>

## 7. Выходные форматы, артефакты и метаданные

* **COCO JSON**: `images`, `annotations` (bbox, category\_id), optional `segmentation` (masks), `info`, `licenses`, `categories`.
* **Кропы и эмбеддинги**: для удобного анализа сохранять crop-изображения и соответствующие эмбеддинги (npz / parquet).
* **Annotation metadata**: для каждой аннотации хранить provenance: источник (GroundingDINO / Qwen / SAM), confidences, ensemble votes, flags (manual\_verified, auto\_rejected). Это критично для отладки и анализа качества.
* **Архивы и версии**: версии COCO (v1, v2), связать с commit hash тренировки, weights metadata.

---

<a name="tools"></a>

## 8. Инструменты и полезные ссылки

* Grounding DINO (text grounding): [https://github.com/IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)
* Segment Anything Model (SAM): [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)
* Grounded-SAM integrations: [https://github.com/IDEA-Research/Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
* Qwen2.5-VL (OpenRouter): [https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct](https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct) & [https://github.com/QwenLM/Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)
* DINOv3 (Meta AI): [https://ai.meta.com/research/publications/dinov3/](https://ai.meta.com/research/publications/dinov3/) & [https://arxiv.org/abs/2508.10104](https://arxiv.org/abs/2508.10104)
* CLIP (OpenAI): [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
* FiftyOne (инспекция/кластеризация): [https://docs.voxel51.com/](https://docs.voxel51.com/)
* Roboflow Blog (аннотации/практики): [https://blog.roboflow.com/](https://blog.roboflow.com/)
* PyImageSearch (про OCR/classification подходы и практики): [https://www.pyimagesearch.com/](https://www.pyimagesearch.com/)
* EasyOCR: [https://github.com/JaidedAI/EasyOCR](https://github.com/JaidedAI/EasyOCR)
* Tesseract OCR: [https://github.com/tesseract-ocr/tesseract](https://github.com/tesseract-ocr/tesseract)

---

<a name="checklist"></a>

## 9. Рекомендованный порядок работ (чек-лист)

1. **Поднять prototype** pipeline: Grounding DINO → SAM на небольшой папке (200–500 изображений).
2. **Собрать первые proposals** (2–3k) и вычислить эмбеддинги (CLIP / DINOv3).
3. **Произвести кластеризацию** в FiftyOne → инспекция representative samples → отловить массовые ошибки.
4. **Настроить ensemble rules** (keep-if-two, confidence thresholds) и запустить вторую итерацию аннотаций.
5. **Сформировать золотой валид-сэт** (500–1000 вручную проверенных примеров).
6. **Сгенерировать синтетику** (несколько тысяч изображений), добавить в train set.
7. **Собрать hard-negatives** (Tinkoff и др.) и обучить лёгкий classifier для post-filtering.
8. **Экспортировать COCO** и подготовить `train/val` сплиты для обучения детектора.
9. **Логировать все версии** аннотаций + provenance metadata.

---
## Scripts in data_preparation/

### Описание скриптов

- **prepare_data_for_colab.py** (в colab_prep/): Создаёт multi-ZIP архивы для загрузки в Colab; упаковывает данные в независимые ZIP для поддиректорий.

- **crop_logos.py** (в synthesis/): Кропит логотипы из официальных референсов по аннотациям COCO; сохраняет кропы в `data_preparation/synthesis/crops/` с именами по классам (purple, white, yellow).

- **gen_synth.py** (в synthesis/): Генерация синтетических изображений с наложением логотипов (3 класса: purple=0, white=1, yellow=2) на фоны. Поддержка аргумента `--N` для количества изображений (default 20). Augmentations с albumentations (brightness/contrast, noise, blur, HSV). Вывод: `data/data_synt/images/labels/{train/val/test}` в YOLO формате.

  Пример:
  ```bash
  python gen_synth.py --N 2000
  ```

- **download_backgrounds.py** (в synthesis/): Скачивает фоны с Picsum (random photos, 640x640, 500 шт.); сохраняет в `data_preparation/synthesis/backgrounds/`. Не требует VPN.

  Пример:
  ```bash
  python download_backgrounds.py
  ```

### Docker for Synth Gen

#### Описание

- **pyproject.toml**, **Dockerfile**, **docker-build-run.bat/sh**, **run_synth.bat** в `synthesis/`: Создаёт изолированное окружение на базе `python:3.10-slim` с uv для deps (albumentations, pillow, numpy). `uv lock && uv sync --frozen`, активирует `.venv`. Копирует `synthesis/` в `/app/synthesis`. Mounts: `/app/synthesis` (crops/backgrounds) и `/app/data` (output `data_synt`). CMD: `python gen_synth.py` (поддержка `--N` или env `N`).

#### Build

- **docker-build-run.bat/sh** (из `data_preparation/synthesis/`): Pull `medphisiker/tbank-synth:latest` и run test с `--N 10`, mounts synthesis/data.

  Пример (sh):
  ```bash
  ./docker-build-run.sh
  ```

- **run_synth.bat** (из корня проекта): Build `tbank-synth` из `synthesis/`, run с mount всего проекта (`/workspace`), env `N` (default 20).

  Пример:
  ```bash
  run_synth.bat 2000
  ```

- Manual build (из `data_preparation/synthesis/`):
  ```bash
  docker build -t tbank-synth -f Dockerfile .
  ```

#### Run

- С mount (из корня проекта, cross-platform с `$(pwd)`; для Windows cmd используйте `%CD%`):
  ```bash
  docker run -v $(pwd)/data_preparation/synthesis:/app/synthesis -v $(pwd)/data:/app/data --rm tbank-synth python gen_synth.py --N 2000
  ```
  Альтернатива env:
  ```bash
  docker run -v $(pwd)/data_preparation/synthesis:/app/synthesis -v $(pwd)/data:/app/data --rm tbank-synth -e N=2000
  ```

  Для Windows (cmd):
  ```cmd
  docker run -v "%CD%/data_preparation/synthesis:/app/synthesis" -v "%CD%/data:/app/data" --rm tbank-synth python gen_synth.py --N 2000
  ```

  Для Windows (PowerShell):
  ```powershell
  docker run -v "${PWD}/data_preparation/synthesis:/app/synthesis" -v "${PWD}/data:/app/data" --rm tbank-synth python gen_synth.py --N 2000
  ```

- Note: Требует Docker Desktop. gen_synth.py использует albumentations для augmentations. Local run:
  ```bash
  cd data_preparation/synthesis && uv sync && python gen_synth.py --N 2000
  ```

#### Publish

```bash
docker login
docker tag tbank-synth medphisiker/tbank-synth:latest
docker push medphisiker/tbank-synth:latest
```

### Использование готового Docker-образа

Для генерации синтетики рекомендуется использовать готовый образ без локальной сборки.

- Pull:
  ```bash
  docker pull medphisiker/tbank-synth:latest
  ```

- Run примеры (аналогично выше, замените `tbank-synth` на `medphisiker/tbank-synth:latest`).

[Готовый образ](https://hub.docker.com/r/medphisiker/tbank-synth).

Примечание: Сгенерированные данные сохраняются в `data/data_synt/`. Поддержка 3 классов логотипов.

### Docker Hub Description

> Docker-образ для генерации синтетических данных логотипов Т-Банка. Включает `gen_synth.py` с albumentations augmentations, uv для deps (albumentations, pillow, numpy). Используйте `docker run` с mounts для `/app/synthesis` и `/app/data`, arg `--N` для количества изображений.
> Пример:
> ```bash
> docker run -v $(pwd)/data_preparation/synthesis:/app/synthesis -v $(pwd)/data:/app/data --rm medphisiker/tbank-synth python gen_synth.py --N 2000
> ```
> Сгенерированные данные сохраняются в `data/data_synt/`. Поддержка 3 классов логотипов.

Категория: Machine Learning or Data Processing.

[Ссылка на Docker Hub](https://hub.docker.com/r/medphisiker/tbank-synth)

### Разработки

- Скрипты предобработки организованы в `data_preparation/` с поддиректориями: `synthesis/`, `yoloe/`, `colab_prep/`, `label_studio/`.
- Независимые ZIP для поддиректорий.
- Автоматическая исправление путей в COCO аннотациях.

## Closing notes

* Цель — максимально автоматизировать и документировать происхождение аннотаций (provenance), чтобы при обнаружении проблем можно было быстро проследить, откуда пришло неверное правило и исправить pipeline.
* На верхнем уровне мы используем современную связку VLM + SAM + verification (VLM/ML classifier/OCR) + эмбеддинг-кластеризацию для масштабируемой и дешёвой по времени разметки.
* Следующий шаг: по этому документу сгенерировать runnable notebook / script в `annotation/` с примером запуска Grounding DINO → SAM → ensemble и с шаблоном промптов. Готов приступить и сгенерировать `annotation`-notebook прямо сейчас, если нужно.

* Цель — максимально автоматизировать и документировать происхождение аннотаций (provenance), чтобы при обнаружении проблем можно было быстро проследить, откуда пришло неверное правило и исправить pipeline.
* На верхнем уровне мы используем современную связку VLM + SAM + verification (VLM/ML classifier/OCR) + эмбеддинг-кластеризацию для масштабируемой и дешёвой по времени разметки.
* Следующий шаг: по этому документу сгенерировать runnable notebook / script в `annotation/` с примером запуска Grounding DINO → SAM → ensemble и с шаблоном промптов. Готов приступить и сгенерировать `annotation`-notebook прямо сейчас, если нужно.
