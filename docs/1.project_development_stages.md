# Project Development Stages — Детекция логотипа Т-Банка

> Файл: `docs/1.project_development_stages.md`
> Краткое назначение: высокоуровневое описание плана действий и итоговых подходов по проекту — от подготовки данных до продакшен-сервиса и валидации.

---

# 1. Цель проекта (кратко)

Построить REST API сервис (порт `8000`, эндпоинт `/detect`), который детектирует логотип **Т-Банка** (стилизованная буква "Т" в щите, цвет может быть любой) на загружаемых изображениях и возвращает абсолютные координаты найденных логотипов в формате Pydantic/COCO. Ограничения: время обработки ≤ 10 секунд на изображение, целевая видеокарта — T4 (16 GB).

---

# 2. Общая архитектурная идея (one-liner)

Полу-автоматическая разметка данных (VLM → SAM → верификация) → обучение и экспорт компактной детекторной модели (YOLOv8 → ONNX/TensorRT FP16) → развязанный FastAPI-фронтенд + отдельный inference-сервис → валидация на «золотом» наборе и мониторинг качества.

---

# 3. Этапы разработки

## 3.1 Подготовка данных (аннотации и валидация)

**Подход:** полу-автоматическая аннотация с использованием современных VLM и масочных моделей.

**Pipeline аннотаций (автомат):**

1. **Grounding DINO** — получение candidate bboxes по промптам (вариации RU/EN: `"stylized letter T inside a shield"`, `"логотип: буква Т в щите"`, и т.п.). Подробнее: репозиторий Grounding DINO — [https://github.com/IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO). ([GitHub][1])
2. **SAM (Segment Anything Model)** — refinement масок и bbox для каждого proposal. Репозиторий SAM — [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything). ([GitHub][2])
3. **Ensemble verification:** использовать Qwen2.5-VL (через OpenRouter) как дополнительную zero-shot проверку и/или валидацию предложений; объединять результаты по простым правилам (например: keep, если два источника согласны или confidence > threshold). OpenRouter — [https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct](https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct); репозиторий Qwen2.5-VL — [https://github.com/QwenLM/Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL). ([OpenRouter][3])
4. Сохранить аннотации в COCO-совместимом формате (bbox + optional mask).

**Обогащение данных:**

* Синтетическая генерация: рендер логотипа в разных цветах/масштабах/углах и наложение на случайные фоны.
* Сбор дополнений из web (поисковые запросы, scraping) — можно автоматизировать и фильтровать через FiftyOne.

**Валидация аннотаций:**

* **Эмбеддинги → кластеризация:** извлечь эмбеддинги кропов (CLIP / DINOv3) и кластеризовать; инспектировать representative samples кластеров (инструменты: FiftyOne / CVAT / Label Studio). FiftyOne docs: [https://docs.voxel51.com/](https://docs.voxel51.com/). ([docs.voxel51.com][4])
* Создать «золотой» валид-сэт (500–1000 вручную проверенных примеров) — этот набор использовать для всех финальных измерений (Precision/Recall/F1 @ IoU=0.5).
* Автоматическая очередь на ручную проверку: low-confidence proposals, слишком маленькие bbox, кластеры с высокой внутрикластерной вариативностью.

**Отличение от похожих брендов (Tinkoff и др.):**

* Пост-фильтр: лёгкий classifier (ResNet18 / EfficientNet-lite) для кропов + OCR (Tesseract / easyOCR) + VLM-верификация (Qwen2.5-VL prompt типа «Is this T-Bank or Tinkoff?»).
* Hard-negative mining: добавить похожие логотипы в negative class при обучении.

---

## 3.2 Обучение детекторной модели

**Подход:** быстрый рабочий baseline, затем оптимизация и экспорт.

* **Baseline модель:** Ultralytics **YOLOv8** — начать с `n`/`s` для прототипа, при необходимости перейти на `m`/`l` для лучшей точности на мелких объектах. Документация YOLOv8: [https://docs.ultralytics.com/](https://docs.ultralytics.com/). ([Ultralytics Docs][5])
* **Тренировочные практики:** multi-scale training; augmentations (mosaic, mixup, color jitter, random occlusion); увеличить input size при проблемах с tiny-объектами (960–1280).
* **DINOv3:** использовать как источник мощных self-supervised эмбеддингов / backbone или для кластеризации и hard-negative mining. Официальная страница DINOv3: [https://ai.meta.com/research/publications/dinov3/](https://ai.meta.com/research/publications/dinov3/) и arXiv: [https://arxiv.org/abs/2508.10104](https://arxiv.org/abs/2508.10104). ([ai.meta.com][6])
* **Экспорт и оптимизация:** экспорт модели в ONNX, последующая генерация TensorRT engine (FP16) для запуска на T4; тестирование latency и memory. (ONNX Runtime: [https://onnxruntime.ai/](https://onnxruntime.ai/); NVIDIA TensorRT: [https://developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt).)

**Метрики:**

* Основная: F1 @ IoU = 0.5 (Precision, Recall, F1).
* Дополнительно: mAP\@0.5:0.95, анализ ошибок по размерам и сценам.

---

## 3.3 Сервисная архитектура (инференс и API)

**Принцип:** отделить фронтенд API от inference-сервиса — гибкость и масштабируемость.

* **Компоненты:**

  1. **FastAPI** (порт `8000`, endpoint `/detect`) — приём изображений, проверка форматов (JPEG/PNG/BMP/WEBP), pre/postprocessing, сбор ответов, возврат Pydantic `DetectionResponse`.
  2. **Inference service** — отдельный сервис (REST/gRPC или Triton/TorchServe/ONNX Runtime) с загруженной моделью (ONNX/TensorRT), отвечает на запросы фронтенда.
  3. **Post-processing:** NMS, фильтры (classifier + OCR + VLM), перевод bbox в абсолютные координаты.
* **Формат ответа:**
  `DetectionResponse` с `detections: List[Detection]`, где `Detection.bbox` содержит `x_min,y_min,x_max,y_max` (int).
* **Docker:** предоставить Dockerfile(ы) для front и inference контейнеров; пример запуска (GPU):

  ```bash
  docker build -t tbank-detector:latest .
  docker run --gpus all -p 8000:8000 --shm-size=1g tbank-detector:latest
  ```

---

# 4. Валидация и метрики

* `evaluate.py`: реализовать matching GT ↔ predictions по IoU (>= 0.5) и считать TP/FP/FN → Precision, Recall, F1.
* Визуализация ошибок: overlay GT (зелёный) / predictions (красный) + score; сохранять кейсы FP/FN для анализа.
* Автоматическое тестирование: запускать evaluate.py на золотом валидационном наборе после каждой значимой тренировки; логирование версий весов и конфигов.

---

# 5. Важные инструменты и ресурсы (ссылки)

* Grounding DINO (text-grounded detection): [https://github.com/IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO). ([GitHub][1])
* Segment Anything Model (SAM): [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything). ([GitHub][2])
* Grounded-SAM (варианты реализций / интеграции): [https://github.com/IDEA-Research/Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything).
* Qwen2.5-VL (через OpenRouter): [https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct](https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct) и репозиторий Qwen2.5-VL: [https://github.com/QwenLM/Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL). ([OpenRouter][3])
* DINOv3 (Meta AI): [https://ai.meta.com/research/publications/dinov3/](https://ai.meta.com/research/publications/dinov3/) и arXiv: [https://arxiv.org/abs/2508.10104](https://arxiv.org/abs/2508.10104). ([ai.meta.com][6])
* Ultralytics YOLOv8 docs: [https://docs.ultralytics.com/](https://docs.ultralytics.com/). ([Ultralytics Docs][5])
* FiftyOne (инспекция, кластеризация, визуализация): [https://docs.voxel51.com/](https://docs.voxel51.com/). ([docs.voxel51.com][4])
* arXiv (научные публикации): [https://arxiv.org/](https://arxiv.org/). ([arxiv.org][7])
* ONNX Runtime: [https://onnxruntime.ai/](https://onnxruntime.ai/)
* NVIDIA TensorRT: [https://developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)

---

# 6. Риски и контрмеры (кратко)

* **Ложные срабатывания похожих логотипов (Tinkoff)** → classifier + OCR + VLM ensemble + hard-negatives.
* **Мелкие объекты (tiny logos)** → увеличить входное разрешение, multi-scale training, при необходимости поднять модель.
* **Плохая авто-разметка** → embedding-кластеризация + выборочная ручная проверка + золотой валид-сэт.
* **Производительность >10s** → FP16 TensorRT, ограничение max side, warm-up, лёгкая модель/оптимизированный inference.

---

# 7. Deliverables (что разместить в репозитории)

* `src/` — FastAPI приложение (эндпоинт `/detect`, Pydantic модели как в ТЗ).
* `train/` — скрипты подготовки COCO, обучения YOLOv8, экспорта в ONNX/TensorRT.
* `annotation/` — pipeline автоматической аннотации (Grounding DINO → merge → SAM → ensemble), примеры промптов и конфиги.
* `validation/` — золотой валид-сет (ссылка/инструкция по скачиванию), `evaluate.py`, визуализации.
* `docker/` — `Dockerfile`(ы) и инструкция запуска.
* `README.md` — полная инструкция по запуску, ссылки на веса и dataset.
* `docs/` — дополнительные markdown-файлы (по каждому компоненту отдельно).

---

# 8. Быстрый чек-лист для старта (минимум работоспособный прототип)

1. Реализовать автоматический pipeline аннотаций и получить первые 2–3k proposals.
2. Кластеризовать эмбеддинги (DINOv3/CLIP) в FiftyOne и провести выборочную ручную проверку → сформировать золотой валид-сэт (500–1000 изображений).
3. Обучить YOLOv8-`s` на полученных аннотациях (baseline).
4. Экспортировать модель в ONNX → протестировать inference на локальном T4 (FP16).
5. Развернуть FastAPI контейнер + inference container, протестировать `/detect` с примерами.
6. Запустить evaluate.py на валид-сете, зафиксировать метрики (Precision/Recall/F1 @ IoU=0.5).

---

# 9. Следующие шаги

* Для каждого компонента (аннотация, обучение, валидация, сервис) создать отдельный markdown с подробной документацией и runnable-скриптами.
* Подготовить `colab`/notebook с примером автоматической аннотации (Grounding DINO → SAM → Qwen2.5-VL ensemble) и шаблонами промптов.
* Подготовить `evaluate.py` и дополнительные utility-скрипты для конвертации в COCO и визуализации.

---

Если хотите, могу сейчас сгенерировать один из следующих артефактов:

* `docs/2.annotation_pipeline.md` — подробное руководство + псевдокод для Grounding DINO → SAM → ensemble;
* `docs/3.training_and_export.md` — подробности по обучению YOLOv8, гиперпараметры и экспорт в ONNX/TensorRT;
* `src/main.py` — пример FastAPI приложения с `/detect` и простым post-processing.

Напишите, какой файл подготовить следующим.

[1]: https://github.com/IDEA-Research/GroundingDINO?utm_source=chatgpt.com "[ECCV 2024] Official implementation of the paper \"Grounding DINO ..."
[2]: https://github.com/facebookresearch/segment-anything?utm_source=chatgpt.com "facebookresearch/segment-anything - GitHub"
[3]: https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct?utm_source=chatgpt.com "Qwen2.5 VL 72B Instruct - API, Providers, Stats | OpenRouter"
[4]: https://docs.voxel51.com/?utm_source=chatgpt.com "FiftyOne — FiftyOne 1.8.0 documentation - Voxel51"
[5]: https://docs.ultralytics.com/?utm_source=chatgpt.com "Home - Ultralytics YOLO Docs"
[6]: https://ai.meta.com/research/publications/dinov3/?utm_source=chatgpt.com "DINOv3 | Research - AI at Meta"
[7]: https://arxiv.org/?utm_source=chatgpt.com "arXiv.org e-Print archive"
