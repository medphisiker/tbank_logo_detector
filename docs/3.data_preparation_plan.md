# Data Preparation — Планы и идеи

> Краткое назначение: высокоуровневое, но практическое руководство по подготовке данных для задачи детекции логотипа Т-Банка — от полу-автоматической аннотации до генерации синтетики и валидации. Этот файл описывает **что** и **почему** делать; точные команды/зависимости будут в скриптах.

---

## Содержание

1. [Overview — идея подхода](#overview)
2. [Автоматический pipeline аннотаций (VLM → SAM → ensemble)](#annotation-pipeline)
3. [Ensemble правила и heuristics](#ensemble)
4. [Фильтрация похожих брендов (Tinkoff) и пост-обработка](#filtering)
5. [Валидация аннотаций: эмбеддинги, кластеризация, золотой валид-сет](#validation)
6. [Синтетика, hard-negatives и аугментации](#synthesis)
7. [Выходные форматы, артефакты и метаданные](#outputs)
8. [Инструменты и полезные ссылки](#tools)
9. [Рекомендованный порядок работ (практический чек-лист)](#checklist)

---

<a name="overview"></a>

## 1. Overview — идея подхода

Цель: минимизировать ручную разметку, получить качественную COCO-аннотацию для обучения детектора и создать воспроизводимую валидацию. Подход:

* Автоматически генерируем proposals текст-запросами с помощью Grounding DINO и/или VLM (Qwen2.5-VL) → уточняем маской через SAM.
* Объединяем (ensemble) результаты разных источников, фильтруем по confidence/IoU/size.
* Применяем пост-фильтрацию похожих логотипов (classifier + OCR + VLM verification).
* Валидируем автоматические аннотации через эмбеддинги (CLIP / DINOv3) и кластеризацию, делаем выборочную ручную проверку; формируем «золотой» валидационный набор (500–1000 изображений).
* Обогащаем датасет синтетикой и hard-negatives, применяем подходящие аугментации.

---

<a name="annotation-pipeline"></a>

## 2. Автоматический pipeline аннотаций (VLM → SAM → ensemble)

### 2.1 Общая последовательность (high-level)

Для каждого изображения:

1. **Grounding DINO** — прогнать с набором промптов (RU/EN) → получить candidate bboxes + confidence.
   Примеры промптов:

   * `"T-bank logo"`
   * `"логотип Т-банка"`
   * `"stylized letter T inside a shield"`
   * `"shield emblem with letter T"`
2. **SAM (Segment Anything Model)** — для каждого bbox передать box prompt → получить маску и более точную рамку.
3. **(Опция)**: дополнительно прогнать **Qwen2.5-VL** (через OpenRouter) с тем же текстовым описанием для подтверждения/оценки bboxes.
4. **Ensemble & heuristics** — слить proposals, применить thresholds и правила объединения (см. раздел 3).
5. Сохранить финальные аннотации в COCO-совместимом формате (bbox + optional mask).

### 2.2 Примечания

* Grounding DINO предназначен для text-grounded detection; SAM даёт качественную маску/корректировку bbox. Синергия этих инструментов («Grounded-SAM») широко применяется для ускорения аннотации.
* Qwen2.5-VL полезен как verifier / second opinion (zero-shot grounding & text-guided verification), особенно у вас есть OpenRouter-доступ.
* Для tiny objects задавайте специальные threshold/size filters (см. ниже).

---

<a name="ensemble"></a>

## 3. Ensemble правила и heuristics

Примеры правил для слияния output’ов нескольких моделей:

* **Keep-if-two**: оставляем bbox, если появилась та же область (IoU >= 0.5) по крайней мере в двух источниках (например, Grounding DINO + Qwen2.5-VL).
* **Confidence threshold**: если bbox от одной модели, но confidence > `C_high` (например 0.85), принимаем.
* **IoU merge**: пересекающиеся proposals (IoU >= 0.6) — объединяем, выбирая bbox как union или weighted average по confidence.
* **Tiny / low confidence rule**: если bbox area < `A_min` (например < 0.01 от площади изображения) и confidence < `C_low` → отправляем в очередь ручной проверки.
* **Size filtering**: отбросить нереалистично большие bbox (логотип обычно не занимает > 50% изображения) или нереалистично маленькие (зависит от задачи).

Параметры `C_high`, `C_low`, `A_min` подбираются эмпирически на первых 2–3k proposals.

---

<a name="filtering"></a>

## 4. Фильтрация похожих брендов (Tinkoff) и пост-обработка

### 4.1 Методы фильтрации

Комбинированный (ensemble) подход даёт наилучшие результаты:

1. **Light classifier**

   * Fine-tune быстрый классификатор (ResNet18 / EfficientNet-lite) на кропах: классы `TBank`, `Tinkoff`, `other`.
   * Это быстрый и недорогой способ отфильтровать большинство ложных срабатываний.

2. **OCR**

   * Выполнить OCR (Tesseract / EasyOCR) вокруг bbox: если найден текст «Тинькофф» → отклонить.
   * Особенно полезно, когда логотип сопровождается брендовым текст.

3. **VLM verification (Qwen2.5-VL / CLIP)**

   * Zero-shot prompt: `"Is this T-Bank logo or Tinkoff?"` — модель выдает вероятности/оценку.
   * CLIP-style similarity: сравнить embedding кропа с эталонными изображениями T-Bank и Tinkoff, выбрать ближайший класс.

4. **Rule ensemble**

   * Пример правила: принять bbox, если хотя бы 2 из 3 методов (detector confidence, classifier, VLM) дают «TBank».

### 4.2 Организация training set для classifier

* Сгенерировать/собрать сбалансированный набор кропов `TBank`, `Tinkoff` и `negatives` (разных размеров/освещения).
* Hard-negative mining: добавить кропы, которые автосистема ошибочно отметила как `TBank`.

---

<a name="validation"></a>

## 5. Валидация аннотаций: эмбеддинги, кластеризация, золотой валид-сет

### 5.1 Кластеризация эмбеддингов

1. Вычислить эмбеддинги для всех proposals (CLIP / DINOv3).
2. Выполнить кластеризацию (k-means / HDBSCAN) — это сгруппирует похожие кропы.
3. Инспектировать *representative samples* из каждого кластера (FiftyOne UI) — быстро выявить массовые ошибки (один большой «плохой» кластер сигнализирует об ошибке pipeline).
4. Для кластеров с высокой внутрикластерной вариативностью — отметить на ручную проверку.

Преимущества: вместо случайной выборки вы проверяете «горячие точки» — экономится время и повышается качество валидации.

### 5.2 Золотой валид-сэт

* Собрать 500–1000 изображений, полностью вручную проверенных и отредактированных (bbox & masks).
* Использовать этот набор для:

  * Итоговой оценки моделей (Precision/Recall/F1 @ IoU=0.5).
  * Тюнинга thresholds и ensemble-правил.
  * Regression testing: каждый релиз модели сравнивать с золотым набором.

---

<a name="synthesis"></a>

## 6. Синтетика, hard-negatives и аугментации

### 6.1 Синтетика

* Рендер векторного логотипа T-Bank в разных цветах/opacity → наклеивание (alpha blending) на случайные фоны (COCO/Unsplash).
* Вариации: разные масштабы, углы поворота, perspective transforms, частичные occlusions (имитация наклейки на поверхность).
* Формировать реальные условия: текстуры, блики, низкое качество JPEG.

Синтетика особенно полезна для повышения вариативности цвета логотипа и агрументации малого набора реальных примеров.

### 6.2 Hard-negatives

* Собрать и пометить похожие логотипы (Tinkoff и другие) и добавить их как negative images (изображения без целевого класса) или отдельный класс для classifier.
* Использовать hard-negative mining: после первой тренировки детектора собрать FP и добавить в train set как negatives.

### 6.3 Аугментации

* Для обучения детектора: rotation, scale, brightness/contrast, blur, jpeg compression, random occlusion/cutout, mosaic/mixup (особенно для YOLO).
* Для classifier: color jitter, small translations, gaussian noise.

---

<a name="outputs"></a>

## 7. Выходные форматы, артефакты и метаданные

* **COCO JSON**: `images`, `annotations` (bbox, category_id), optional `segmentation` (masks), `info`, `licenses`, `categories`.
* **Кропы и эмбеддинги**: для удобного анализа сохранять crop-изображения и соответствующие эмбеддинги (npz / parquet).
* **Annotation metadata**: для каждой аннотации хранить provenance: источник (GroundingDINO / Qwen / SAM), confidences, ensemble votes, flags (manual_verified, auto_rejected). Это критично для отладки и анализа качества.
* **Архивы и версии**: версии COCO (v1, v2), связать с commit hash тренировки, weights metadata.

---

<a name="tools"></a>

## 8. Инструменты и полезные ссылки

* Grounding DINO (text grounding): [https://github.com/IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)
* Segment Anything Model (SAM): [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)
* Grounded-SAM integrations: [https://github.com/IDEA-Research/Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
* Qwen2.5-VL (OpenRouter): [https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct](https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct) & [https://github.com/QwenLM/Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)
* DINOv3 (Meta AI): [https://ai.meta.com/research/publications/dinov3/](https://ai.meta.com/research/publications/dinov3/) & [https://arxiv.org/abs/2508.10104](https://arxiv.org/abs/2508.10104)
* CLIP (OpenAI): [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
* FiftyOne (инспекция/кластеризация): [https://docs.voxel51.com/](https://docs.voxel51.com/)
* Roboflow Blog (аннотации/практики): [https://blog.roboflow.com/](https://blog.roboflow.com/)
* PyImageSearch (про OCR/classification подходы и практики): [https://www.pyimagesearch.com/](https://www.pyimagesearch.com/)
* EasyOCR: [https://github.com/JaidedAI/EasyOCR](https://github.com/JaidedAI/EasyOCR)
* Tesseract OCR: [https://github.com/tesseract-ocr/tesseract](https://github.com/tesseract-ocr/tesseract)

---

<a name="checklist"></a>

## 9. Рекомендованный порядок работ (чек-лист)

1. **Поднять prototype** pipeline: Grounding DINO → SAM на небольшой папке (200–500 изображений).
2. **Собрать первые proposals** (2–3k) и вычислить эмбеддинги (CLIP / DINOv3).
3. **Произвести кластеризацию** в FiftyOne → инспекция representative samples → отловить массовые ошибки.
4. **Настроить ensemble rules** (keep-if-two, confidence thresholds) и запустить вторую итерацию аннотаций.
5. **Сформировать золотой валид-сэт** (500–1000 вручную проверенных примеров).
6. **Сгенерировать синтетику** (несколько тысяч изображений), добавить в train set.
7. **Собрать hard-negatives** (Tinkoff и др.) и обучить лёгкий classifier для post-filtering.
8. **Экспортировать COCO** и подготовить `train/val` сплиты для обучения детектора.
9. **Логировать все версии** аннотаций + provenance metadata.

---

Для реализации скриптов и практических команд см. [3.data_preparation_scripts.md](3.data_preparation_scripts.md).